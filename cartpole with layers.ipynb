{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
       "array([[ 0.00523372,  0.02015132,  0.00278229, -0.04800152]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array(env.reset()[0])\n",
    "\n",
    "state_tensor = tf.convert_to_tensor(state)\n",
    "state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "\n",
    "state_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create linear layer\n",
    "\n",
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super().__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units), initializer='random_normal', trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(units,), initializer='zeros', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model with custom layer\n",
    "\n",
    "class MLP(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observations = Linear(4)\n",
    "        self.hidden = Linear(32)\n",
    "        self.action = Linear(2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.observations(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.hidden(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.action(x)\n",
    "\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "n_observations = 4\n",
    "n_actions = 2\n",
    "n_hidden = 128\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    observations = layers.Input(shape=(n_observations,))\n",
    "    hidden = layers.Dense(n_hidden, activation='relu')(observations)\n",
    "    action = layers.Dense(n_actions, activation='softmax')(hidden)\n",
    "    \n",
    "    return keras.Model(inputs=observations, outputs=action)\n",
    "\n",
    "policy_network = create_q_model()\n",
    "target_network = create_q_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_history = []\n",
    "rewards_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "done_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9e392708071f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy_network' is not defined"
     ]
    }
   ],
   "source": [
    "epsilon = 1.0\n",
    "\n",
    "for timestep in range(10000):\n",
    "\n",
    "    if epsilon > np.random.rand(1)[0]:\n",
    "        action = np.random.choice(2)\n",
    "    else:\n",
    "        state_tensor = tf.convert_to_tensor(state)\n",
    "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "        action_probs = policy_network(state_tensor, training=False)\n",
    "        action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "    epsilon = epsilon*0.99\n",
    "    \n",
    "    state_next, reward, done, _, _= env.step(action)\n",
    "    \n",
    "    action_history.append(action)\n",
    "    rewards_history.append(reward)\n",
    "    state_history.append(state)\n",
    "    state_next_history.append(state_next)\n",
    "    done_history.append(done)\n",
    "        \n",
    "    state_next = np.array(state_next)\n",
    "    state = state_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
